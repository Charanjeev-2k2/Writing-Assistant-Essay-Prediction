{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Essay Text Prediction",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charanjeev-2k2/Writing-Assistant-Essay-Prediction/blob/main/Essay_Text_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8gZX6hMs_XH"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz_Ef1ahf9lT"
      },
      "source": [
        "import tensorflow \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd7r-OZotGTf"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLWEw6YAuTJj"
      },
      "source": [
        "import string\n",
        "file = open(\"Essays for text prediction model.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "  \n",
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "\n",
        "#Replace \\n,\\r,\\ufeff with spaces   \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "\n",
        "#Replace punctuation with spaces\n",
        "translator_string = data.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator_string)\n",
        "\n",
        "#Removing repeating words\n",
        "help = []\n",
        "for i in new_data.split():\n",
        "    if (i not in help):\n",
        "        help.append(i)\n",
        "\n",
        "# help = new_data.split()\n",
        "# help = list(set(help))\n",
        "data = ' '.join(help)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Viv5tPum20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c5d41e-7395-4fdf-bea1-3dc95568b92c"
      },
      "source": [
        "#TOKENIZE\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 65, 1017, 66, 67, 1018, 1019, 68, 1020, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGnUxpMFgSW3",
        "outputId": "3f42e58e-e219-4232-ba21-12378bad3924"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "type(sequences[-2][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DamHpQwEjS0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb881b3c-7870-459b-a2ed-7d216a41f354"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "      X.append(i[0])\n",
        "      y.append(i[1])\n",
        "    \n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc6_gPQjH-zX"
      },
      "source": [
        "#Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eHHyX0RM4A5"
      },
      "source": [
        "#Creating the model\n",
        "import keras\n",
        "from keras import layers \n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(keras.layers.LSTM(256, return_sequences=True))\n",
        "model.add(keras.layers.LSTM(256))\n",
        "model.add(keras.layers.Dense(1000, activation=\"tanh\"))\n",
        "model.add(layers.Dense(vocab_size, activation=\"softmax\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF_fTBQNPPv3",
        "outputId": "b170bf32-a838-40ec-fb6e-c5818c6c5a10"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1, 10)             117670    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 1, 256)            273408    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1000)              257000    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 11767)             11778767  \n",
            "=================================================================\n",
            "Total params: 12,952,157\n",
            "Trainable params: 12,952,157\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B5vsOyBcE0N"
      },
      "source": [
        "#Implementing callbacks\n",
        "import tensorflow \n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dz0U5WHd1yE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b551a1e2-5b36-4f86-8838-4d47c372705e"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=0.01))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI8OSx7ifozI",
        "outputId": "2f8c95b3-0cbd-4beb-8d73-f360db324691"
      },
      "source": [
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "201/201 [==============================] - 41s 184ms/step - loss: 9.4466\n",
            "\n",
            "Epoch 00001: loss improved from inf to 9.54594, saving model to nextword1.h5\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 37s 183ms/step - loss: 9.5614\n",
            "\n",
            "Epoch 00002: loss improved from 9.54594 to 9.38086, saving model to nextword1.h5\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 36s 180ms/step - loss: 8.4813\n",
            "\n",
            "Epoch 00003: loss improved from 9.38086 to 8.56670, saving model to nextword1.h5\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 36s 180ms/step - loss: 7.2435\n",
            "\n",
            "Epoch 00004: loss improved from 8.56670 to 7.52159, saving model to nextword1.h5\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 6.2872\n",
            "\n",
            "Epoch 00005: loss improved from 7.52159 to 6.64474, saving model to nextword1.h5\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 5.4679\n",
            "\n",
            "Epoch 00006: loss improved from 6.64474 to 5.85986, saving model to nextword1.h5\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 4.7841\n",
            "\n",
            "Epoch 00007: loss improved from 5.85986 to 5.17304, saving model to nextword1.h5\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 4.1250\n",
            "\n",
            "Epoch 00008: loss improved from 5.17304 to 4.52259, saving model to nextword1.h5\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 36s 180ms/step - loss: 3.5244\n",
            "\n",
            "Epoch 00009: loss improved from 4.52259 to 3.93528, saving model to nextword1.h5\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 3.0251\n",
            "\n",
            "Epoch 00010: loss improved from 3.93528 to 3.44322, saving model to nextword1.h5\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 36s 177ms/step - loss: 2.6412\n",
            "\n",
            "Epoch 00011: loss improved from 3.44322 to 2.99310, saving model to nextword1.h5\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 2.2073\n",
            "\n",
            "Epoch 00012: loss improved from 2.99310 to 2.55241, saving model to nextword1.h5\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 36s 177ms/step - loss: 1.8857\n",
            "\n",
            "Epoch 00013: loss improved from 2.55241 to 2.19776, saving model to nextword1.h5\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 36s 177ms/step - loss: 1.6361\n",
            "\n",
            "Epoch 00014: loss improved from 2.19776 to 1.87449, saving model to nextword1.h5\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 38s 188ms/step - loss: 1.3502\n",
            "\n",
            "Epoch 00015: loss improved from 1.87449 to 1.58091, saving model to nextword1.h5\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 1.1148\n",
            "\n",
            "Epoch 00016: loss improved from 1.58091 to 1.30177, saving model to nextword1.h5\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.9841\n",
            "\n",
            "Epoch 00017: loss improved from 1.30177 to 1.12089, saving model to nextword1.h5\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.8001\n",
            "\n",
            "Epoch 00018: loss improved from 1.12089 to 0.95720, saving model to nextword1.h5\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.7233\n",
            "\n",
            "Epoch 00019: loss improved from 0.95720 to 0.87454, saving model to nextword1.h5\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.7517\n",
            "\n",
            "Epoch 00020: loss improved from 0.87454 to 0.86457, saving model to nextword1.h5\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.7353\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.86457\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 36s 177ms/step - loss: 0.8079\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.86457\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 36s 177ms/step - loss: 0.8727\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.86457\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.6022\n",
            "\n",
            "Epoch 00024: loss improved from 0.86457 to 0.55769, saving model to nextword1.h5\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.2522\n",
            "\n",
            "Epoch 00025: loss improved from 0.55769 to 0.28536, saving model to nextword1.h5\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.2084\n",
            "\n",
            "Epoch 00026: loss improved from 0.28536 to 0.25292, saving model to nextword1.h5\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 36s 180ms/step - loss: 0.2085\n",
            "\n",
            "Epoch 00027: loss improved from 0.25292 to 0.24696, saving model to nextword1.h5\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1999\n",
            "\n",
            "Epoch 00028: loss improved from 0.24696 to 0.23772, saving model to nextword1.h5\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.1937\n",
            "\n",
            "Epoch 00029: loss improved from 0.23772 to 0.23754, saving model to nextword1.h5\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 36s 177ms/step - loss: 0.2028\n",
            "\n",
            "Epoch 00030: loss improved from 0.23754 to 0.23324, saving model to nextword1.h5\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 35s 176ms/step - loss: 0.2038\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.23324\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 37s 185ms/step - loss: 0.1924\n",
            "\n",
            "Epoch 00032: loss improved from 0.23324 to 0.23202, saving model to nextword1.h5\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 36s 180ms/step - loss: 0.1951\n",
            "\n",
            "Epoch 00033: loss improved from 0.23202 to 0.23156, saving model to nextword1.h5\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1978\n",
            "\n",
            "Epoch 00034: loss improved from 0.23156 to 0.23078, saving model to nextword1.h5\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1978\n",
            "\n",
            "Epoch 00035: loss improved from 0.23078 to 0.23068, saving model to nextword1.h5\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.1981\n",
            "\n",
            "Epoch 00036: loss improved from 0.23068 to 0.22922, saving model to nextword1.h5\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 36s 181ms/step - loss: 0.1972\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.22922\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.2007\n",
            "\n",
            "Epoch 00038: loss improved from 0.22922 to 0.22854, saving model to nextword1.h5\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.1946\n",
            "\n",
            "Epoch 00039: loss improved from 0.22854 to 0.22794, saving model to nextword1.h5\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 36s 180ms/step - loss: 0.1888\n",
            "\n",
            "Epoch 00040: loss improved from 0.22794 to 0.22692, saving model to nextword1.h5\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1867\n",
            "\n",
            "Epoch 00041: loss improved from 0.22692 to 0.22519, saving model to nextword1.h5\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.1917\n",
            "\n",
            "Epoch 00042: loss improved from 0.22519 to 0.22502, saving model to nextword1.h5\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1817\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.22502\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1924\n",
            "\n",
            "Epoch 00044: loss improved from 0.22502 to 0.22327, saving model to nextword1.h5\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 36s 178ms/step - loss: 0.1934\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.22327\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1828\n",
            "\n",
            "Epoch 00046: loss improved from 0.22327 to 0.22173, saving model to nextword1.h5\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 36s 179ms/step - loss: 0.1943\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.22173\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 38s 188ms/step - loss: 0.1854\n",
            "\n",
            "Epoch 00048: loss improved from 0.22173 to 0.21996, saving model to nextword1.h5\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 36s 180ms/step - loss: 0.1865\n",
            "\n",
            "Epoch 00049: loss improved from 0.21996 to 0.21878, saving model to nextword1.h5\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 36s 181ms/step - loss: 0.1861\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.21878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f940026f250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSyOlQisEfXH",
        "outputId": "8e09a9d2-15b2-4d01-b76b-6d0cf94ae783"
      },
      "source": [
        "loss = model.evaluate(X, y)\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "402/402 [==============================] - 12s 31ms/step - loss: 0.1570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15703359246253967"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D9CDl-sIFm0"
      },
      "source": [
        "#Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhOiZsdWVIJ5"
      },
      "source": [
        "# Importing the Libraries\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "model = load_model('nextword1.h5')\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "        In this function we are using the tokenizer and models trained\n",
        "        and we are creating the sequence of the text entered and then\n",
        "        using our model to predict and return the the predicted word.\n",
        "    \n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        preds = model.predict_classes(sequence)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REoix66lVRD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4f47f8-30e4-445e-8f86-54fd6c524d27"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "    Testing our model until the user decides to stop the script.\n",
        "    While the script is running we try and check if \n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "\n",
        "            text = ''.join(text)\n",
        "            Predict_Next_Words(model, tokenizer, text)\n",
        "            \n",
        "        except:\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your line: this\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "might\n",
            "Enter your line: what\n",
            "regarded\n",
            "Enter your line: where\n",
            "shallower\n",
            "Enter your line: that\n",
            "distinction\n",
            "Enter your line: stop the script\n",
            "Ending The Program.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmYTcuuw_VXm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}